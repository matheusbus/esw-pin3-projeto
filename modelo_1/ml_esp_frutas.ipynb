{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fbdc6dd-9bd7-4b67-9e31-81a5147a109d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-20 16:26:31.404568: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-20 16:26:31.502337: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-20 16:26:31.914941: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-20 16:26:34.090554: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "260cbcc7-285c-4d54-b24d-327f00d2e1a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contagem de imagens de morango: 306\n",
      "Contagem de imagens de pêssego: 304\n",
      "Contagem de imagens de romã: 311\n"
     ]
    }
   ],
   "source": [
    "## 1. Dataset das imagens\n",
    "dataset_dir = os.path.join(os.getcwd(), 'imagens')\n",
    "dataset_morango_dir = os.path.join(dataset_dir, 'morango')\n",
    "dataset_pessego_dir = os.path.join(dataset_dir, 'pessego')\n",
    "dataset_roma_dir = os.path.join(dataset_dir, 'roma')\n",
    "\n",
    "dataset_morango_len = len(os.listdir(dataset_morango_dir))\n",
    "dataset_pessego_len = len(os.listdir(dataset_pessego_dir))\n",
    "dataset_roma_len = len(os.listdir(dataset_roma_dir))\n",
    "\n",
    "print(f'Contagem de imagens de morango: {dataset_morango_len}')\n",
    "print(f'Contagem de imagens de pêssego: {dataset_pessego_len}')\n",
    "print(f'Contagem de imagens de romã: {dataset_roma_len}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "202294f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Aplicando o rebalanceamento do dataset\n",
    "# -> Utilizada a técnica de 'data augmentation' para gerar novas imagens de morango e pessego\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "\n",
    "# Criar um objeto ImageDataGenerator para data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,          # Faixa de rotação aleatória em graus\n",
    "    width_shift_range=0.2,      # Faixa de deslocamento horizontal aleatório (como uma fração da largura total)\n",
    "    height_shift_range=0.2,     # Faixa de deslocamento vertical aleatório (como uma fração da altura total)\n",
    "    shear_range=0.2,            # Faixa de cisalhamento aleatório (em radianos)\n",
    "    zoom_range=0.2,             # Faixa de zoom aleatório\n",
    "    horizontal_flip=True,      # Inverter aleatoriamente as imagens horizontalmente (espelhamento)\n",
    "    fill_mode='nearest'        # Estratégia de preenchimento usada para preencher novos pixels gerados após a rotação ou deslocamento\n",
    ")\n",
    "\n",
    "if (dataset_morango_len == 250) or (dataset_pessego_len == 250): \n",
    "    numero_imagens_aumentadas = 61\n",
    "    classes_aumentadas = ['morango', 'pessego']\n",
    "\n",
    "    for classe in classes_aumentadas:\n",
    "        # Diretório da classe atual\n",
    "        diretorio_classe = os.path.join(dataset_dir, classe)\n",
    "        \n",
    "        # Criar diretório para a classe aumentada, se não existir\n",
    "        diretorio_destino_classe = os.path.join(dataset_dir, classe)\n",
    "        os.makedirs(diretorio_destino_classe, exist_ok=True)\n",
    "        \n",
    "        # Lista de arquivos de imagem na classe atual\n",
    "        imagens_classe = os.listdir(diretorio_classe)\n",
    "        \n",
    "        # Selecionar aleatoriamente algumas imagens existentes para data augmentation\n",
    "        indices_amostra = np.random.choice(len(imagens_classe), numero_imagens_aumentadas, replace=True)\n",
    "        \n",
    "        # Para cada imagem de amostra selecionada\n",
    "        for indice in indices_amostra:\n",
    "            imagem_nome = imagens_classe[indice]\n",
    "            imagem_path = os.path.join(diretorio_classe, imagem_nome)\n",
    "            \n",
    "            # Carregar imagem\n",
    "            imagem = Image.open(imagem_path)\n",
    "            imagem_array = np.array(imagem)\n",
    "            imagem_array = imagem_array.reshape((1,) + imagem_array.shape)  # Reshape para (1, altura, largura, canais)\n",
    "            \n",
    "            # Gerar imagens aumentadas e salvar no diretório de destino\n",
    "            for i, batch in enumerate(datagen.flow(imagem_array, batch_size=1)):\n",
    "                if i >= 1:  # Quantidade de imagens aumentadas a serem geradas por imagem de amostra\n",
    "                    break\n",
    "                imagem_aumentada = batch[0].astype(np.uint8)  # Converter de volta para o formato de imagem\n",
    "                nova_imagem_nome = f\"{os.path.splitext(imagem_nome)[0]}_aug_{i}.jpg\"\n",
    "                nova_imagem_path = os.path.join(diretorio_destino_classe, nova_imagem_nome)\n",
    "                nova_imagem = Image.fromarray(imagem_aumentada)\n",
    "                nova_imagem.save(nova_imagem_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f11c96a-7b60-4ece-b705-d472939d7169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Definição e separação dos dados de treinamento e dados de teste\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "# Diretório do projeto\n",
    "projeto_dir = os.getcwd()\n",
    "\n",
    "# Caminhos para as pastas de treinamento, validação e teste\n",
    "dataset_treinamento_dir = os.path.join(projeto_dir, 'imagens_treinamento')\n",
    "dataset_validacao_dir = os.path.join(projeto_dir, 'imagens_validacao')\n",
    "dataset_teste_dir = os.path.join(projeto_dir, 'imagens_teste')\n",
    "\n",
    "# Proporções para dividir os dados entre treinamento, validação e teste\n",
    "proporcao_treinamento = 0.6  # 60% para treinamento\n",
    "proporcao_validacao = 0.2  # 20% para validação\n",
    "proporcao_teste = 0.2  # 20% para teste\n",
    "\n",
    "# Conforme conversado com professor, utilizar a função random para gerar o random_seed\n",
    "#random_seed = random.randint(1, 99)\n",
    "\n",
    "# No entanto, estou utilizando um valor setado em 42 para ter sempre os mesmos conjuntos de dados durante os testes do código\n",
    "random_seed = 42\n",
    "\n",
    "classes = ['morango', 'pessego', 'roma']\n",
    "\n",
    "# Função que executará a separação das imagens (utiliza algoritmo de aleatoriedade)\n",
    "def split_dataset(classe):\n",
    "    dataset_treinamento_classe_dir = os.path.join(dataset_treinamento_dir, classe) # Dir. destino treinamento classe\n",
    "    dataset_teste_classe_dir = os.path.join(dataset_teste_dir, classe) # Dir. destino teste classe\n",
    "\n",
    "    # Dir. origem imagens classe\n",
    "    dataset_classe_dir = os.path.join(projeto_dir, 'imagens', classe)\n",
    "    imagens_classe = [os.path.join(dataset_classe_dir, img) for img in os.listdir(dataset_classe_dir)] # Popular lista com imagens\n",
    "\n",
    "    imagens_treinamento, imagens_temp = train_test_split(imagens_classe, test_size=proporcao_validacao + proporcao_teste, random_state=random_seed)\n",
    "    imagens_validacao, imagens_teste = train_test_split(imagens_temp, test_size=proporcao_teste / (proporcao_validacao + proporcao_teste), random_state=random_seed)\n",
    "\n",
    "    # Criação dos diretórios para as classes\n",
    "    os.makedirs(os.path.join(dataset_treinamento_dir, classe), exist_ok=True)\n",
    "    os.makedirs(os.path.join(dataset_validacao_dir, classe), exist_ok=True)\n",
    "    os.makedirs(os.path.join(dataset_teste_dir, classe), exist_ok=True)\n",
    "    \n",
    "    imagens_classe = [os.path.join(dataset_classe_dir, img) for img in os.listdir(dataset_classe_dir)]\n",
    "\n",
    "    # Copia as imagens para as pastas correspondentes\n",
    "    for imagem in imagens_treinamento:\n",
    "        shutil.copy(imagem, os.path.join(dataset_treinamento_dir, classe))\n",
    "    for imagem in imagens_validacao:\n",
    "        shutil.copy(imagem, os.path.join(dataset_validacao_dir, classe))\n",
    "    for imagem in imagens_teste:\n",
    "        shutil.copy(imagem, os.path.join(dataset_teste_dir, classe))\n",
    "\n",
    "# Verifica se os diretórios de treinamento, validação e teste já existem\n",
    "if not os.path.exists(dataset_treinamento_dir) or not os.path.exists(dataset_validacao_dir) or not os.path.exists(dataset_teste_dir):\n",
    "    # Cria os diretórios\n",
    "    os.makedirs(dataset_treinamento_dir, exist_ok=True)\n",
    "    os.makedirs(dataset_validacao_dir, exist_ok=True)\n",
    "    os.makedirs(dataset_teste_dir, exist_ok=True)\n",
    "\n",
    "    # Para cada classe, executa a função de separação de dados\n",
    "    for classe in classes:\n",
    "        split_dataset(classe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60c7c32a-e4e8-435b-b67d-0e8ee0e51f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contagem de imagens de morango para treinamento: 183\n",
      "Contagem de imagens de morango para teste: 62\n",
      "Contagem de imagens de morango para validacao: 61\n",
      "Contagem de imagens de pêssego para treinamento: 182\n",
      "Contagem de imagens de pêssego para teste: 61\n",
      "Contagem de imagens de pêssego para validacao: 61\n",
      "Contagem de imagens de romã para treinamento: 186\n",
      "Contagem de imagens de romã para teste: 63\n",
      "Contagem de imagens de romã para validacao: 62\n"
     ]
    }
   ],
   "source": [
    "# Função para contar o número de imagens de cada diretório\n",
    "def contar_imagens(diretorio):\n",
    "    return sum([len(files) for root, dirs, files in os.walk(diretorio)])\n",
    "\n",
    "# Contagem de imagens para treinamento e teste de cada classe\n",
    "dataset_treinamento_morango_len = contar_imagens(os.path.join(dataset_treinamento_dir, 'morango'))\n",
    "dataset_teste_morango_len = contar_imagens(os.path.join(dataset_teste_dir, 'morango'))\n",
    "dataset_validacao_morango_len = contar_imagens(os.path.join(dataset_validacao_dir, 'morango'))\n",
    "dataset_treinamento_pessego_len = contar_imagens(os.path.join(dataset_treinamento_dir, 'pessego'))\n",
    "dataset_teste_pessego_len = contar_imagens(os.path.join(dataset_teste_dir, 'pessego'))\n",
    "dataset_validacao_pessego_len = contar_imagens(os.path.join(dataset_validacao_dir, 'pessego'))\n",
    "dataset_treinamento_roma_len = contar_imagens(os.path.join(dataset_treinamento_dir, 'roma'))\n",
    "dataset_teste_roma_len = contar_imagens(os.path.join(dataset_teste_dir, 'roma'))\n",
    "dataset_validacao_roma_len = contar_imagens(os.path.join(dataset_validacao_dir, 'roma'))\n",
    "\n",
    "# Impressão das contagens\n",
    "print(f'Contagem de imagens de morango para treinamento: {dataset_treinamento_morango_len}')\n",
    "print(f'Contagem de imagens de morango para teste: {dataset_teste_morango_len}')\n",
    "print(f'Contagem de imagens de morango para validacao: {dataset_validacao_morango_len}')\n",
    "print(f'Contagem de imagens de pêssego para treinamento: {dataset_treinamento_pessego_len}')\n",
    "print(f'Contagem de imagens de pêssego para teste: {dataset_teste_pessego_len}')\n",
    "print(f'Contagem de imagens de pêssego para validacao: {dataset_validacao_pessego_len}')\n",
    "print(f'Contagem de imagens de romã para treinamento: {dataset_treinamento_roma_len}')\n",
    "print(f'Contagem de imagens de romã para teste: {dataset_teste_roma_len}')\n",
    "print(f'Contagem de imagens de romã para validacao: {dataset_validacao_roma_len}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e820b6a-891f-4d1d-8934-59dc4f5c427f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/matheus/Documentos/GitHub/esw-pin3-projeto/modelo_1/imagens_treinamento\n",
      "/home/matheus/Documentos/GitHub/esw-pin3-projeto/modelo_1/imagens_teste\n",
      "Found 551 files belonging to 3 classes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 184 files belonging to 3 classes.\n",
      "Found 186 files belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "## 4. Pré-processamento das imagens\n",
    "## --> Definir tamanho de entrada das minhas imagens (em px)\n",
    "## --> Definir qual estratégia de conversão adotar (scaling da imagem / foco no centro da imagem ignorando periferia / recortar imagem até no limite do tamanho definido e ignorar o restante)\n",
    "\n",
    "'''\n",
    "Considerações sobre a abordagem:\n",
    "O pré-processamento é feito pela função \"tf.keras.preprocessing.image_dataset_from_directory\". Essa função permite especificar o tamanho das imagens e aplica automaticamente a normalização durante o carregamento das imagens. Além disso, aplica embaralhamento\n",
    "e carrega as imagens em lotes (batch). Por fim, as imagens são consideradas como tensores do TensorFlow.\n",
    "'''\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "image_width = 300\n",
    "image_heigth = 300\n",
    "image_size = (image_width, image_heigth)\n",
    "\n",
    "image_color_channel = 3\n",
    "image_color_channel_size = 255\n",
    "image_shape = image_size + (image_color_channel,)\n",
    "\n",
    "batch_size = 32 # Valor que vou puxar do dataset por vez\n",
    "epoch = 20 # Quantidade de vezes que vou percorrer meu dataset inteiro\n",
    "learning_rate = 0.0001 # Taxa de aprendizagem\n",
    "\n",
    "classes = ['morango', 'pessego', 'roma']\n",
    "\n",
    "print(dataset_treinamento_dir)\n",
    "print(dataset_teste_dir)\n",
    "\n",
    "data_set_treinamento = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    dataset_treinamento_dir,\n",
    "    image_size = image_size,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True, # Embaralhamento\n",
    "    label_mode='categorical' # Carrega os dados em formato one-hot\n",
    ")\n",
    "\n",
    "data_set_validacao = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    dataset_validacao_dir,\n",
    "    image_size = image_size,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = False,\n",
    "    label_mode='categorical'\n",
    ")\n",
    "\n",
    "data_set_teste = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    dataset_teste_dir,\n",
    "    image_size = image_size,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = False, # Não é necessário embaralhar os dados de teste\n",
    "    label_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8fe27dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Definição da arquitetura da rede neural do modelo\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, Input\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
    "\n",
    "def create_model(learning_rate, optimizer):\n",
    "  \"\"\"\n",
    "  Cria um modelo de rede neural convolucional com parâmetros específicos.\n",
    "\n",
    "  Args:\n",
    "      learning_rate (float): A taxa de aprendizado para o otimizador especificado.\n",
    "      optimizer (str): O nome do otimizador a ser utilizado ('adam', 'sgd', ou 'rmsprop').\n",
    "\n",
    "  Returns:\n",
    "      tf.keras.models.Sequential: O modelo criado.\n",
    "  \"\"\"\n",
    "\n",
    "  # Define o otimizador com base no parâmetro passado\n",
    "  if optimizer == 'adam':\n",
    "      optimizer = Adam(learning_rate)\n",
    "  elif optimizer == 'sgd':\n",
    "      optimizer = SGD(learning_rate)\n",
    "  elif optimizer == 'rmsprop':\n",
    "      optimizer = RMSprop(learning_rate)\n",
    "  else:\n",
    "      raise ValueError(\"Otimizador inválido. Escolha entre 'adam', 'sgd' ou 'rmsprop'.\")\n",
    "\n",
    "  # Arquitetura da rede do modelo implementado:\n",
    "  arc_model = Sequential([\n",
    "    # Definição do tipo de entrada: imagens de 300x300 pixels com 3 canais de cores (RGB) \n",
    "    Input(shape=(300,300,3)),\n",
    "\n",
    "    # Primeira camada: convolucional com 32 filtros de tamanho 3x3, utilizando a função de ativação ReLU\n",
    "    # Cada filtro aprende padrões diferentes de cada imagem.\n",
    "    # Tamanho 3x3 significa que o filtro vai processar com fragmentos de 3x3 pixels\n",
    "    # Função de ativação: introduz não linearidade na rede.\n",
    "    Conv2D(32, (3,3), activation='relu'),\n",
    "\n",
    "    # Camada de normalização de batch, para normalizar a ativação da camada anterior\n",
    "    # Ajuda a estabilizar e normalizar as ativações durante o treinamento.\n",
    "    BatchNormalization(),\n",
    "\n",
    "    # Camada de MaxPooling, realiza downsampling (compressão) da entrada\n",
    "    # Processa em tamanho de 2x2 pixels \n",
    "    MaxPooling2D((2,2)),\n",
    "\n",
    "    # Mais uma camada Conv2D, com 64 filtros de tamanho 3x3, com função de ativação ReLU\n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "\n",
    "    # Outra camada de normalização de batch, para normalizar a ativação da camada anterior\n",
    "    BatchNormalization(),\n",
    "\n",
    "    # Outra Camda de MaxPooling\n",
    "    MaxPooling2D((2,2)),\n",
    "\n",
    "    # Camada de achatamento (Flatten) para transformar os mapas de características 2D em um vetor 1D\n",
    "    # Esta camada prepara os dados para entrada nas camadas densas (abaixo)\n",
    "    Flatten(),\n",
    "    \n",
    "    # Camada densa (totalmente conectada) com 128 neurônios e função de ativação ReLU\n",
    "    Dense(128, activation='relu'),\n",
    "    \n",
    "    # Mias uma camada de normalização de batch, para normalizar a ativação da camada anterior\n",
    "    BatchNormalization(),\n",
    "\n",
    "    # Mais uma camada densa com 64 neurônios e função de ativação ReLU\n",
    "    Dense(64, activation='relu'),\n",
    "    \n",
    "    # Camada de Dropout para prevenir overfitting, desativando aleatoriamente 50% dos neurônios\n",
    "    Dropout(0.5),\n",
    "    \n",
    "    # Camada de saída com 3 neurônios (um para cada classe) e função de ativação softmax para a classificação multiclasse\n",
    "    # A saída é uma distribuição de probabilidade sobre as classes\n",
    "    Dense(3, activation='softmax')\n",
    "  ])\n",
    "\n",
    "  # Compilação do modelo\n",
    "  # Aqui, utilizamos a função de perda conforme espeficicado no documento do projeto\n",
    "  # Função de perda: crossentropy, que irá calcular a diferença entre as previsões realizadas pelo modelo e os rótulos verdadeiros associados aos dados de treinamento.\n",
    "  arc_model.compile(optimizer=optimizer, loss=CategoricalCrossentropy(), metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Além disso, testaremos 3 tipos de funções de otimização:\n",
    "\n",
    "# Modelo com função de otimização Adam\n",
    "model_adam = create_model(optimizer='adam', learning_rate=0.0001)\n",
    "\n",
    "# Modelo com função de otimização SGD\n",
    "model_sgd = create_model(optimizer='sgd', learning_rate=0.0001)\n",
    "\n",
    "# Modelo com função de otimização RMSprop\n",
    "model_rmsprop = create_model(optimizer='sgd', learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3bc18158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/stepep - accuracy: 0.9291 - loss: 0.239\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "Confusion Matrix: \n",
      "[[60  0  1]\n",
      " [39 21  1]\n",
      " [55  2  5]]\n",
      "Accuracy:  0.4673913043478261\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 7s/step - accuracy: 0.9285 - loss: 0.2399 - val_accuracy: 0.4674 - val_loss: 1.6694\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-21 00:27:01.176675: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/stepep - accuracy: 0.9483 - loss: 0.200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 880ms/step\n",
      "Confusion Matrix: \n",
      "[[59  0  2]\n",
      " [18 32 11]\n",
      " [25  5 32]]\n",
      "Accuracy:  0.6684782608695652\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 7s/step - accuracy: 0.9479 - loss: 0.1999 - val_accuracy: 0.6685 - val_loss: 0.7523\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-21 00:29:01.808134: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/stepep - accuracy: 0.9584 - loss: 0.146\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "Confusion Matrix: \n",
      "[[61  0  0]\n",
      " [12 41  8]\n",
      " [24  5 33]]\n",
      "Accuracy:  0.7336956521739131\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 7s/step - accuracy: 0.9586 - loss: 0.1459 - val_accuracy: 0.7337 - val_loss: 0.6708\n",
      "Epoch 4/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-21 00:31:02.957270: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/stepep - accuracy: 0.9776 - loss: 0.117\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 906ms/step\n",
      "Confusion Matrix: \n",
      "[[60  0  1]\n",
      " [ 5 56  0]\n",
      " [20 15 27]]\n",
      "Accuracy:  0.7771739130434783\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 7s/step - accuracy: 0.9774 - loss: 0.1175 - val_accuracy: 0.7772 - val_loss: 0.6451\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-21 00:33:05.317795: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/stepep - accuracy: 0.9735 - loss: 0.099\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 869ms/step\n",
      "Confusion Matrix: \n",
      "[[56  3  2]\n",
      " [ 1 59  1]\n",
      " [ 6 22 34]]\n",
      "Accuracy:  0.8097826086956522\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 7s/step - accuracy: 0.9736 - loss: 0.0995 - val_accuracy: 0.8098 - val_loss: 0.5577\n",
      "Epoch 6/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-21 00:35:05.427898: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/stepep - accuracy: 0.9505 - loss: 0.169\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 936ms/step\n",
      "Confusion Matrix: \n",
      "[[55  3  3]\n",
      " [ 3 56  2]\n",
      " [ 9 18 35]]\n",
      "Accuracy:  0.7934782608695652\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 6s/step - accuracy: 0.9513 - loss: 0.1660 - val_accuracy: 0.7935 - val_loss: 0.4810\n",
      "Epoch 7/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-21 00:36:59.344207: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/stepep - accuracy: 0.9789 - loss: 0.097\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 881ms/step\n",
      "Confusion Matrix: \n",
      "[[56  2  3]\n",
      " [ 2 57  2]\n",
      " [11 12 39]]\n",
      "Accuracy:  0.8260869565217391\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 6s/step - accuracy: 0.9786 - loss: 0.0972 - val_accuracy: 0.8261 - val_loss: 0.4642\n",
      "Epoch 8/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-21 00:38:54.013726: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/stepep - accuracy: 0.9960 - loss: 0.047\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 926ms/step\n",
      "Confusion Matrix: \n",
      "[[58  1  2]\n",
      " [ 0 58  3]\n",
      " [11 12 39]]\n",
      "Accuracy:  0.842391304347826\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 7s/step - accuracy: 0.9959 - loss: 0.0471 - val_accuracy: 0.8424 - val_loss: 0.4690\n",
      "Epoch 9/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-21 00:40:56.219085: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/stepep - accuracy: 0.9979 - loss: 0.049\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 883ms/step\n",
      "Confusion Matrix: \n",
      "[[56  1  4]\n",
      " [ 0 53  8]\n",
      " [ 5  7 50]]\n",
      "Accuracy:  0.8641304347826086\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 7s/step - accuracy: 0.9976 - loss: 0.0500 - val_accuracy: 0.8641 - val_loss: 0.3927\n",
      "Epoch 10/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-21 00:42:55.378616: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/stepep - accuracy: 0.9984 - loss: 0.031\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 961ms/step\n",
      "Confusion Matrix: \n",
      "[[54  5  2]\n",
      " [ 0 60  1]\n",
      " [ 7 21 34]]\n",
      "Accuracy:  0.8043478260869565\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 7s/step - accuracy: 0.9984 - loss: 0.0320 - val_accuracy: 0.8043 - val_loss: 0.4882\n",
      "Epoch 11/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-21 00:44:58.761062: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/stepep - accuracy: 0.9932 - loss: 0.037\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 938ms/step\n",
      "Confusion Matrix: \n",
      "[[53  6  2]\n",
      " [ 0 60  1]\n",
      " [11 15 36]]\n",
      "Accuracy:  0.8097826086956522\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 7s/step - accuracy: 0.9931 - loss: 0.0382 - val_accuracy: 0.8098 - val_loss: 0.4950\n",
      "Epoch 12/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-21 00:46:56.155270: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/stepep - accuracy: 0.9960 - loss: 0.037\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 876ms/step\n",
      "Confusion Matrix: \n",
      "[[54  4  3]\n",
      " [ 0 60  1]\n",
      " [ 7 20 35]]\n",
      "Accuracy:  0.8097826086956522\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 7s/step - accuracy: 0.9960 - loss: 0.0371 - val_accuracy: 0.8098 - val_loss: 0.5144\n",
      "Epoch 13/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-21 00:48:53.552491: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/stepep - accuracy: 0.9994 - loss: 0.028\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 905ms/step\n",
      "Confusion Matrix: \n",
      "[[48  7  6]\n",
      " [ 0 55  6]\n",
      " [ 3 10 49]]\n",
      "Accuracy:  0.8260869565217391\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 6s/step - accuracy: 0.9994 - loss: 0.0287 - val_accuracy: 0.8261 - val_loss: 0.4434\n",
      "Epoch 14/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-21 00:50:49.569352: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/stepep - accuracy: 0.9992 - loss: 0.022\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 904ms/step\n",
      "Confusion Matrix: \n",
      "[[54  3  4]\n",
      " [ 1 55  5]\n",
      " [ 5 10 47]]\n",
      "Accuracy:  0.8478260869565217\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 6s/step - accuracy: 0.9991 - loss: 0.0228 - val_accuracy: 0.8478 - val_loss: 0.4331\n",
      "Epoch 15/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-21 00:52:45.391778: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/stepep - accuracy: 1.0000 - loss: 0.018\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 889ms/step\n",
      "Confusion Matrix: \n",
      "[[57  2  2]\n",
      " [ 1 53  7]\n",
      " [10 10 42]]\n",
      "Accuracy:  0.8260869565217391\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 6s/step - accuracy: 1.0000 - loss: 0.0182 - val_accuracy: 0.8261 - val_loss: 0.4248\n",
      "Epoch 16/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-21 00:54:41.113642: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/stepep - accuracy: 0.9952 - loss: 0.023\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 903ms/step\n",
      "Confusion Matrix: \n",
      "[[53  3  5]\n",
      " [ 1 56  4]\n",
      " [ 4 11 47]]\n",
      "Accuracy:  0.8478260869565217\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 7s/step - accuracy: 0.9953 - loss: 0.0240 - val_accuracy: 0.8478 - val_loss: 0.4378\n",
      "Epoch 17/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-21 00:56:38.223983: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/stepep - accuracy: 1.0000 - loss: 0.014\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 934ms/step\n",
      "Confusion Matrix: \n",
      "[[52  3  6]\n",
      " [ 0 55  6]\n",
      " [ 3  7 52]]\n",
      "Accuracy:  0.8641304347826086\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 6s/step - accuracy: 1.0000 - loss: 0.0149 - val_accuracy: 0.8641 - val_loss: 0.4129\n",
      "Epoch 18/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-21 00:58:34.140834: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/stepep - accuracy: 0.9992 - loss: 0.016\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 880ms/step\n",
      "Confusion Matrix: \n",
      "[[54  2  5]\n",
      " [ 0 55  6]\n",
      " [ 5 10 47]]\n",
      "Accuracy:  0.8478260869565217\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 7s/step - accuracy: 0.9991 - loss: 0.0164 - val_accuracy: 0.8478 - val_loss: 0.4623\n",
      "Epoch 19/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-21 01:00:31.725129: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/stepep - accuracy: 0.9984 - loss: 0.017\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 895ms/step\n",
      "Confusion Matrix: \n",
      "[[52  2  7]\n",
      " [ 0 53  8]\n",
      " [ 5  7 50]]\n",
      "Accuracy:  0.842391304347826\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 6s/step - accuracy: 0.9984 - loss: 0.0172 - val_accuracy: 0.8424 - val_loss: 0.4328\n",
      "Epoch 20/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-21 01:02:27.091712: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/stepep - accuracy: 0.9855 - loss: 0.031\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 881ms/step\n",
      "Confusion Matrix: \n",
      "[[44  4 13]\n",
      " [ 0 44 17]\n",
      " [ 2  5 55]]\n",
      "Accuracy:  0.7771739130434783\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 6s/step - accuracy: 0.9859 - loss: 0.0315 - val_accuracy: 0.7772 - val_loss: 0.5368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-21 01:04:23.444508: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nEpochs: Cada linha corresponde a uma época de treinamento. Uma época é uma iteração completa sobre todo o conjunto de dados de treinamento.\\n\\nBatches por Época: Aqui, o número 23/23 indica que há 23 batches (ou lotes) de dados sendo processados em cada época. Isso pode variar dependendo do tamanho do conjunto de treinamento e do tamanho do lote (batch_size). Por exemplo, se você tem 700 amostras de treinamento e está usando um tamanho de lote de 32, então cada época terá 22 batches (700/32 = 21.875).\\n\\nTempo por Época: O tempo indicado representa o tempo gasto para completar uma época. Isso pode variar dependendo da complexidade do modelo, tamanho do conjunto de dados e recursos do hardware.\\n\\nAccuracy (Acurácia) e Loss (Perda): A acurácia e a perda durante o treinamento e validação são mostradas para cada época. A acurácia indica a proporção de previsões corretas em relação ao total de exemplos, enquanto a perda (loss) é uma medida do quão bem o modelo está performando durante o treinamento, sendo minimizada ao longo das épocas.\\n\\nVal_accuracy e Val_loss: Estes são os valores de acurácia e perda calculados no conjunto de dados de validação, que é usado para avaliar o desempenho do modelo em dados que não foram vistos durante o treinamento. É importante monitorar essas métricas para evitar overfitting e garantir que o modelo esteja generalizando bem para novos dados.\\n\\nTest accuracy e Test loss: Após o treinamento, o modelo é avaliado no conjunto de dados de teste (geralmente separado do conjunto de validação). Estas métricas fornecem uma avaliação final do desempenho do modelo em dados completamente novos e não vistos durante o treinamento.\\n\\nAcurácia: No final do treinamento, a acurácia nos dados de validação foi de aproximadamente 84.41%. Isso significa que o modelo classificou corretamente cerca de 84.41% das amostras nos dados de validação.\\n\\nPerda: A perda nos dados de validação foi de aproximadamente 0.4956. A perda é uma medida de quão boa ou ruim é a previsão do modelo para uma única amostra; valores mais baixos são melhores.\\n\\nTendência ao longo das épocas: A acurácia e a perda parecem estar se estabilizando ao longo das épocas, o que sugere que o modelo está convergindo para uma solução.\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6. Treinamento do modelo e algumas validações com accuray e confusion matrix\n",
    "\n",
    "# Declaração da função de callback que será chamada no fim do processamento de cada época durante o treinamento\n",
    "# Calcula a matriz de confusão e teste de acurácia\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import numpy as np \n",
    "\n",
    "class MetricsCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, dados_validacao):\n",
    "        self.dados_validacao = dados_validacao\n",
    "\n",
    "    # Vai ser chamado no final de cada época durante o treinamento\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        \n",
    "        # Iterando sobre os dados para obter os rótulos verdadeiros e preditos\n",
    "        for x_batch, y_batch in self.dados_validacao:\n",
    "            y_true.extend(np.argmax(y_batch, axis=1))\n",
    "\n",
    "            y_pred_batch = np.argmax(self.model.predict(x_batch), axis=1)\n",
    "            y_pred.extend(y_pred_batch)\n",
    "\n",
    "        # Calculando a matriz de confusão\n",
    "        cm = confusion_matrix(y_true=y_true, y_pred=y_pred)\n",
    "\n",
    "        # Calculando a acurácia\n",
    "        accuracy = accuracy_score(y_true=y_true, y_pred=y_pred)\n",
    "\n",
    "        # Impressão dos resultados\n",
    "        print(\"Matriz de confusão: \")\n",
    "        print(cm)\n",
    "        print('Acurácia da epoch: ', accuracy)\n",
    "\n",
    "\n",
    "# Instância das métricas (acurácia e matriz de confusão)\n",
    "metrics_callback = MetricsCallback(data_set_validacao)\n",
    "\n",
    "\n",
    "# Treinamento do modelo com função de otimização Adam\n",
    "# É passado o dataset de treinamento, a quantidade de épocas (quantas vezes vai percorrer o dataset) e o dataset de validação\n",
    "model_adam.fit(\n",
    "    data_set_treinamento,\n",
    "    epochs=epoch,\n",
    "    validation_data=data_set_validacao,\n",
    "    callbacks=[metrics_callback]\n",
    ")\n",
    "\n",
    "# Teste de acurácia com o dataset de testes\n",
    "teste_perca, teste_acuracia = model_adam.evaluate(data_set_teste)\n",
    "print('Teste acurácia com dataset de testes: ', teste_acuracia)\n",
    "\n",
    "'''\n",
    "\n",
    "Epochs: Cada linha corresponde a uma época de treinamento. Uma época é uma iteração completa sobre todo o conjunto de dados de treinamento.\n",
    "\n",
    "Batches por Época: Aqui, o número 23/23 indica que há 23 batches (ou lotes) de dados sendo processados em cada época. Isso pode variar dependendo do tamanho do conjunto de treinamento e do tamanho do lote (batch_size). Por exemplo, se você tem 700 amostras de treinamento e está usando um tamanho de lote de 32, então cada época terá 22 batches (700/32 = 21.875).\n",
    "\n",
    "Tempo por Época: O tempo indicado representa o tempo gasto para completar uma época. Isso pode variar dependendo da complexidade do modelo, tamanho do conjunto de dados e recursos do hardware.\n",
    "\n",
    "Accuracy (Acurácia) e Loss (Perda): A acurácia e a perda durante o treinamento e validação são mostradas para cada época. A acurácia indica a proporção de previsões corretas em relação ao total de exemplos, enquanto a perda (loss) é uma medida do quão bem o modelo está performando durante o treinamento, sendo minimizada ao longo das épocas.\n",
    "\n",
    "Val_accuracy e Val_loss: Estes são os valores de acurácia e perda calculados no conjunto de dados de validação, que é usado para avaliar o desempenho do modelo em dados que não foram vistos durante o treinamento. É importante monitorar essas métricas para evitar overfitting e garantir que o modelo esteja generalizando bem para novos dados.\n",
    "\n",
    "Test accuracy e Test loss: Após o treinamento, o modelo é avaliado no conjunto de dados de teste (geralmente separado do conjunto de validação). Estas métricas fornecem uma avaliação final do desempenho do modelo em dados completamente novos e não vistos durante o treinamento.\n",
    "\n",
    "Acurácia: No final do treinamento, a acurácia nos dados de validação foi de aproximadamente 84.41%. Isso significa que o modelo classificou corretamente cerca de 84.41% das amostras nos dados de validação.\n",
    "\n",
    "Perda: A perda nos dados de validação foi de aproximadamente 0.4956. A perda é uma medida de quão boa ou ruim é a previsão do modelo para uma única amostra; valores mais baixos são melhores.\n",
    "\n",
    "Tendência ao longo das épocas: A acurácia e a perda parecem estar se estabilizando ao longo das épocas, o que sugere que o modelo está convergindo para uma solução.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "43ee338f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "GridSearch.__init__() got multiple values for argument 'objective'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m hyperparameters\u001b[38;5;241m.\u001b[39mInt(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m, min_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, max_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, step\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Define o tuner GridSearch\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m tuner \u001b[38;5;241m=\u001b[39m \u001b[43mGridSearch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_adam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# Função que constrói o modelo\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhyperparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Hiperparâmetros a serem otimizados\u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobjective\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval_accuracy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m           \u001b[49m\u001b[38;5;66;43;03m# Número total de tentativas de modelo (combinatórias)\u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdirectory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgrid_search\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Diretório para salvar resultados do tuner\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproject_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgrid_search_example\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Nome do projeto\u001b[39;49;00m\n\u001b[1;32m     16\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Realiza a busca\u001b[39;00m\n\u001b[1;32m     19\u001b[0m tuner\u001b[38;5;241m.\u001b[39msearch(data_set_treinamento, validation_data\u001b[38;5;241m=\u001b[39mdata_set_teste)\n",
      "\u001b[0;31mTypeError\u001b[0m: GridSearch.__init__() got multiple values for argument 'objective'"
     ]
    }
   ],
   "source": [
    "from keras_tuner.tuners import GridSearch\n",
    "from keras_tuner.engine.hyperparameters import HyperParameters\n",
    "\n",
    "# Define os hiperparâmetros\n",
    "hyperparameters = HyperParameters()\n",
    "hyperparameters.Int('epochs', min_value=10, max_value=50, step=10)\n",
    "\n",
    "# Define o tuner GridSearch\n",
    "tuner = GridSearch(\n",
    "    model_adam,            # Função que constrói o modelo\n",
    "    hyperparameters,        # Hiperparâmetros a serem otimizados\n",
    "    max_trials=9,           # Número total de tentativas de modelo (combinatórias)\n",
    "    directory='grid_search', # Diretório para salvar resultados do tuner\n",
    "    project_name='grid_search_example' # Nome do projeto\n",
    ")\n",
    "\n",
    "# Realiza a busca\n",
    "tuner.search(data_set_treinamento, validation_data=data_set_teste)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
